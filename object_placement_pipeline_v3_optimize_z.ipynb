{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from diffusers.utils import make_image_grid\n",
    "from transformers import (\n",
    "    #     SamModel,\n",
    "    #     SamProcessor,\n",
    "    #     Blip2Processor,\n",
    "    #     Blip2ForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "from src.eunms import Model_Type, Scheduler_Type\n",
    "from src.utils.enums_utils import get_pipes\n",
    "from src.config import RunConfig\n",
    "from main import run as invert\n",
    "\n",
    "from attention_maps_utils_by_timesteps import (\n",
    "    get_attn_maps,\n",
    "    cross_attn_init,\n",
    "    register_cross_attention_hook,\n",
    "    set_layer_with_name_and_path,\n",
    "    preprocess,\n",
    "    visualize_and_save_attn_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'safety_checker': None} are not expected by StableDiffusionXLImg2ImgOptimizeZPipeline and will be ignored.\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  3.75it/s]/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.88it/s]\n",
      "Keyword arguments {'safety_checker': None} are not expected by StableDiffusionXLImg2ImgPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_type = Model_Type.SDXL_Turbo\n",
    "scheduler_type = Scheduler_Type.EULER\n",
    "pipe_inversion, pipe_inference = get_pipes(\n",
    "    model_type, scheduler_type, device=device, is_optimize_z=True\n",
    ")\n",
    "_, pipe_extract_attn_maps = get_pipes(model_type, scheduler_type, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunConfig(\n",
    "    model_type=model_type,\n",
    "    scheduler_type=scheduler_type,\n",
    "    noise_regularization_lambda_kl=0.08,\n",
    "    noise_regularization_lambda_ac=40,\n",
    "    num_inversion_steps=4,\n",
    "    num_inference_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/object_placement_images.csv\"\n",
    ")\n",
    "data = pd.read_csv(data_path, dtype={\"bg_img_id\": str, \"ref_img_id\": str})\n",
    "open_images_path = \"/cortex/data/images/OpenImagesV6/images\"\n",
    "cocoee_path = \"/cortex/data/images/COCOEE/test_bench/Ref_3500\"\n",
    "\n",
    "# open_images dataset is used for bg_images. add a column of the path to the image (the path is the <open_images_path>/<image_id>.jpg)\n",
    "data[\"bg_img_path\"] = data[\"bg_img_id\"].apply(lambda x: f\"{open_images_path}/{x}.jpg\")\n",
    "\n",
    "# cocoee dataset is used for ref_images. add a column of the path to the image (the path is the <cocoee_path>/<image_id>_ref.png)\n",
    "data[\"ref_img_path\"] = data[\"ref_img_id\"].apply(lambda x: f\"{cocoee_path}/{x}_ref.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bg_images = [\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/bed.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/desk.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/cabinet.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/face.jpg\",\n",
    "]\n",
    "\n",
    "my_object_images = [\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/pillow/pillow.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/plant/plant.jpg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/vase/vase.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/hat/hat.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_images = list(data[\"bg_img_path\"])\n",
    "bg_images.extend(my_bg_images)\n",
    "ref_images = list(data[\"ref_img_path\"])\n",
    "ref_images.extend(my_object_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return image.convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "\n",
    "bg_images = [preprocess_image(image) for image in bg_images]\n",
    "ref_images = [preprocess_image(image) for image in ref_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "#     \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
    "# ).to(device)\n",
    "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\"\n",
    ").to(device)\n",
    "llava_processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(processor, model, images, object_names=None):\n",
    "    if object_names is None:\n",
    "        propmt = \"USER: <image>\\nWhat's in the image (answer in shortest way possible)? ASSISTANT:\"\n",
    "        prompts = [propmt for _ in range(len(images))]\n",
    "    else:\n",
    "        prompts = [\n",
    "            f\"USER: <image>\\nWhat's in the image that a {ref} can be on (answer in shortest way possible)? ASSISTANT:\"\n",
    "            for ref in object_names\n",
    "        ]\n",
    "    answers = []\n",
    "    for prompt, image in zip(prompts, images):\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "        generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
    "        answer = processor.batch_decode(\n",
    "            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        # the answer should be the string after the last colon\n",
    "        answer = answer.split(\":\")[-1].strip()\n",
    "        answers.append(answer)\n",
    "    return answers\n",
    "\n",
    "\n",
    "def get_edit_prompts(inversion_prompts, objects):\n",
    "    edit_prompts = []\n",
    "    for inversion_prompt, object in zip(inversion_prompts, objects):\n",
    "        edit_prompts.append(f\"a {inversion_prompt} and a {object}\".lower())\n",
    "    return edit_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_names = get_prompts(llava_processor, llava_model, ref_images)\n",
    "inversion_prompts = [\"an image\" for _ in range(len(bg_images))]\n",
    "edit_prompts = [f\"an image of a {o.lower()}\" for o in object_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: an image + Glass -> an image of a glass\n",
      "Prompt: an image + Glass -> an image of a glass\n",
      "Prompt: an image + Pepsi -> an image of a pepsi\n",
      "Prompt: an image + Cat -> an image of a cat\n",
      "Prompt: an image + Vase -> an image of a vase\n",
      "Prompt: an image + Orange -> an image of a orange\n",
      "Prompt: an image + Glass -> an image of a glass\n",
      "Prompt: an image + Plant -> an image of a plant\n",
      "Prompt: an image + Suitcase -> an image of a suitcase\n",
      "Prompt: an image + Teddy bear -> an image of a teddy bear\n",
      "Prompt: an image + Dog -> an image of a dog\n",
      "Prompt: an image + Glass -> an image of a glass\n",
      "Prompt: an image + Pillow -> an image of a pillow\n",
      "Prompt: an image + Plant -> an image of a plant\n",
      "Prompt: an image + Vase -> an image of a vase\n",
      "Prompt: an image + Hat -> an image of a hat\n"
     ]
    }
   ],
   "source": [
    "for inversion_prompt, object_name, edit_prompt in zip(\n",
    "    inversion_prompts, object_names, edit_prompts\n",
    "):\n",
    "    print(f\"Prompt: {inversion_prompt} + {object_name} -> {edit_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_images(images, prompts):\n",
    "    inv_latents = []\n",
    "    noises = []\n",
    "    for image, prompt in zip(images, prompts):\n",
    "        _, inv_latent, noise, _ = invert(\n",
    "            image,\n",
    "            prompt,\n",
    "            config,\n",
    "            pipe_inversion=pipe_inversion,\n",
    "            pipe_inference=pipe_inference,\n",
    "            do_reconstruction=False,\n",
    "        )\n",
    "        inv_latents.append(inv_latent)\n",
    "        noises.append(noise)\n",
    "    return inv_latents, noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "inv_latents, noises = invert_images(bg_images, inversion_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_init()\n",
    "pipe_inference.unet = set_layer_with_name_and_path(pipe_inference.unet)\n",
    "pipe_inference.unet = register_cross_attention_hook(pipe_inference.unet)\n",
    "pipe_extract_attn_maps.unet = set_layer_with_name_and_path(pipe_extract_attn_maps.unet)\n",
    "pipe_extract_attn_maps.unet = register_cross_attention_hook(pipe_extract_attn_maps.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt2tokens(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    tokens = []\n",
    "    for text_input_id in text_input_ids[0]:\n",
    "        token = tokenizer.decoder[text_input_id.item()]\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_attn_map(timestep_attn_maps, tokenizer, prompt):\n",
    "    preprocessed_attn_maps = {}\n",
    "    for k, v in timestep_attn_maps.items():\n",
    "        v = torch.mean(v.cpu(), axis=0).squeeze(0)\n",
    "        if v.shape[-1] != 16:\n",
    "            continue\n",
    "        preprocessed_attn_maps[k] = v\n",
    "    attn_map = torch.stack(list(preprocessed_attn_maps.values()), axis=0)\n",
    "    attn_map = torch.mean(attn_map, axis=0)\n",
    "\n",
    "    tokens = prompt2tokens(tokenizer, prompt)\n",
    "    eos_token = tokenizer.eos_token\n",
    "    for token, token_attn_map in zip(tokens, attn_map):\n",
    "        if token != eos_token:\n",
    "            continue\n",
    "        return token_attn_map\n",
    "\n",
    "\n",
    "def get_attn_map(edit_prompt, tokenizer):\n",
    "    attn_maps = get_attn_maps()\n",
    "    attn_map = process_attn_map(attn_maps[-1], tokenizer, edit_prompt)\n",
    "    return attn_map\n",
    "\n",
    "\n",
    "def reset_attn_maps():\n",
    "    attn_maps = get_attn_maps()\n",
    "    attn_maps.clear()\n",
    "\n",
    "\n",
    "def extract_attn_maps(inv_latents, noises, inversion_prompts, edit_prompts):\n",
    "    edit_images = []\n",
    "    last_timestep_attn_maps = []\n",
    "    for inv_latent, noise, inversion_prompt, edit_prompt in zip(\n",
    "        inv_latents, noises, inversion_prompts, edit_prompts\n",
    "    ):\n",
    "        pipe_extract_attn_maps.scheduler.set_noise_list(noise)\n",
    "        pipe_extract_attn_maps.cfg = config\n",
    "        edit_image = pipe_extract_attn_maps(\n",
    "            prompt=edit_prompt,\n",
    "            num_inference_steps=config.num_inference_steps,\n",
    "            negative_prompt=inversion_prompt,\n",
    "            image=inv_latent,\n",
    "            strength=config.inversion_max_step,\n",
    "            denoising_start=1.0 - config.inversion_max_step,\n",
    "            guidance_scale=config.guidance_scale,\n",
    "        ).images[0]\n",
    "        edit_images.append(edit_image)\n",
    "        last_timestep_attn_maps.append(\n",
    "            get_attn_map(edit_prompt, pipe_extract_attn_maps.tokenizer)\n",
    "        )\n",
    "        reset_attn_maps()\n",
    "\n",
    "    return last_timestep_attn_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_attn_map(edit_prompt):\n",
    "#     attn_maps = get_attn_maps()\n",
    "#     attn_map = preprocess(attn_maps[-1], 512, 512)\n",
    "#     attn_map_img = visualize_and_save_attn_map(\n",
    "#         attn_map, pipe_inference.tokenizer, edit_prompt, edit_prompt.split()[-1].lower()\n",
    "#     )\n",
    "#     return attn_map_img\n",
    "\n",
    "\n",
    "# def reset_attn_maps():\n",
    "#     attn_maps = get_attn_maps()\n",
    "#     attn_maps.clear()\n",
    "\n",
    "\n",
    "def get_edit_images(\n",
    "    inv_latents, noises, inversion_prompts, edit_prompts, all_attn_maps\n",
    "):\n",
    "    edit_images = []\n",
    "    last_timestep_attn_maps = []\n",
    "    for inv_latent, noise, inversion_prompt, edit_prompt, sample_attn_maps in zip(\n",
    "        inv_latents, noises, inversion_prompts, edit_prompts, all_attn_maps\n",
    "    ):\n",
    "        pipe_inference.scheduler.set_noise_list(noise)\n",
    "        pipe_inference.cfg = config\n",
    "        edit_image = pipe_inference(\n",
    "            prompt=edit_prompt,\n",
    "            general_attn_map=sample_attn_maps,\n",
    "            num_inference_steps=config.num_inference_steps,\n",
    "            negative_prompt=inversion_prompt,\n",
    "            image=inv_latent,\n",
    "            strength=config.inversion_max_step,\n",
    "            denoising_start=1.0 - config.inversion_max_step,\n",
    "            guidance_scale=config.guidance_scale,\n",
    "        ).images[0]\n",
    "        edit_images.append(edit_image)\n",
    "        # last_timestep_attn_maps.append(get_attn_map(edit_prompt))\n",
    "\n",
    "    return edit_images, last_timestep_attn_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.86it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.01it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.94it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.13it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.93it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.15it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.12it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.15it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.20it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.94it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.91it/s]\n"
     ]
    }
   ],
   "source": [
    "extracted_attn_maps = extract_attn_maps(\n",
    "    inv_latents, noises, inversion_prompts, edit_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_attn_maps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_images, last_timestep_attn_maps = get_edit_images(\n",
    "    inv_latents, noises, inversion_prompts, edit_prompts, extracted_attn_maps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_x_y(attn_map):\n",
    "    max_index = np.argmax(attn_map)\n",
    "    max_x = max_index % attn_map.shape[1]\n",
    "    max_y = max_index // attn_map.shape[1]\n",
    "    return max_x, max_y\n",
    "\n",
    "\n",
    "# def get_placement_mask(edit_image):\n",
    "def get_placement_mask(attn_map):\n",
    "    # max_x, max_y = get_max_x_y(np.array(edit_image))\n",
    "    max_x, max_y = get_max_x_y(attn_map)\n",
    "    sam_input_points = [[[max_x, max_y]]]\n",
    "    sam_inputs = sam_processor(\n",
    "        Image.fromarray(attn_map).convert(\"RGB\"),\n",
    "        input_points=sam_input_points,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    # sam_inputs = sam_processor(edit_image, input_points=sam_input_points, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = sam_model(**sam_inputs)\n",
    "\n",
    "    masks = (\n",
    "        sam_processor.image_processor.post_process_masks(\n",
    "            outputs.pred_masks.cpu(),\n",
    "            sam_inputs[\"original_sizes\"].cpu(),\n",
    "            sam_inputs[\"reshaped_input_sizes\"].cpu(),\n",
    "        )[0]\n",
    "        .squeeze()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    mask = Image.fromarray(masks[0].astype(np.uint8) * 255)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_placement_mask_on_bg_image(bg_image, placement_mask):\n",
    "    image_np = np.array(bg_image)\n",
    "    mask_np = np.array(placement_mask)\n",
    "\n",
    "    masked_image_np = np.copy(image_np)\n",
    "    masked_image_np[mask_np == 255] = [\n",
    "        255,\n",
    "        0,\n",
    "        0,\n",
    "    ]  # Set RGB values to [255, 0, 0] for red\n",
    "\n",
    "    masked_image = Image.fromarray(masked_image_np)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_images = []\n",
    "masks = []\n",
    "for bg_image, edit_image, attn_map in zip(\n",
    "    bg_images, edit_images, last_timestep_attn_maps\n",
    "):\n",
    "    # placement_mask = get_placement_mask(np.array(attn_map))\n",
    "    placement_mask = get_placement_mask(edit_image)\n",
    "    masked_image = get_placement_mask_on_bg_image(bg_image, placement_mask)\n",
    "    masked_images.append(masked_image)\n",
    "    masks.append(placement_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anydoor_images_path = \"/home/lab/yairshp/projects/third_party/AnyDoor/results/seg\"\n",
    "anydoor_images_paths = [\n",
    "    f\"{anydoor_images_path}/{f}\" for f in os.listdir(anydoor_images_path)\n",
    "]\n",
    "anydoor_images = [Image.open(image_path) for image_path in anydoor_images_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_grid_arr = []\n",
    "for bg_image, ref_image, edit_image, attn_map, masked_image, anydoor_image in zip(\n",
    "    bg_images,\n",
    "    ref_images,\n",
    "    edit_images,\n",
    "    last_timestep_attn_maps,\n",
    "    masked_images,\n",
    "    anydoor_images,\n",
    "):\n",
    "    images_grid_arr.append(bg_image)\n",
    "    images_grid_arr.append(ref_image)\n",
    "    images_grid_arr.append(edit_image)\n",
    "    images_grid_arr.append(attn_map)\n",
    "    images_grid_arr.append(masked_image)\n",
    "    images_grid_arr.append(anydoor_image)\n",
    "make_image_grid(images_grid_arr, len(bg_images), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_map = last_timestep_attn_maps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(attn_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = []\n",
    "for mask in masks:\n",
    "    mask_arr = np.array(mask)\n",
    "    non_zero_rows, non_zero_cols = np.nonzero(mask_arr == 255)\n",
    "    top_left_row = np.min(non_zero_rows)\n",
    "    top_left_col = np.min(non_zero_cols)\n",
    "    bottom_right_row = np.max(non_zero_rows)\n",
    "    bottom_right_col = np.max(non_zero_cols)\n",
    "    bbox = np.zeros_like(mask)\n",
    "    # bbox[top_left_row, top_left_col:bottom_right_col+1] = 255\n",
    "    # bbox[bottom_right_row, top_left_col:bottom_right_col+1] = 255\n",
    "    # bbox[top_left_row:bottom_right_row+1, top_left_col] = 255\n",
    "    # bbox[top_left_row:bottom_right_row+1, bottom_right_col] = 255\n",
    "    bbox[top_left_row : bottom_right_row + 1, top_left_col : bottom_right_col + 1] = 255\n",
    "    bounding_boxes.append(Image.fromarray(bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = []\n",
    "for bbox, mask in zip(bounding_boxes, masks):\n",
    "    xxx.append(bbox)\n",
    "    xxx.append(mask)\n",
    "make_image_grid(xxx, len(bounding_boxes), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (object_name, mask, bbox) in enumerate(zip(object_names, masks, bounding_boxes)):\n",
    "    mask.save(\n",
    "        f\"/home/lab/yairshp/projects/insert_object/data/seg_mask_{object_name}_{i}.png\"\n",
    "    )\n",
    "    bbox.save(\n",
    "        f\"/home/lab/yairshp/projects/insert_object/data/bbox_{object_name}_{i}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
