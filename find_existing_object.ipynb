{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from diffusers.utils import make_image_grid\n",
    "from transformers import (\n",
    "    SamModel,\n",
    "    SamProcessor,\n",
    "    Blip2Processor,\n",
    "    Blip2ForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "from src.eunms import Model_Type, Scheduler_Type\n",
    "from src.utils.enums_utils import get_pipes\n",
    "from src.config import RunConfig\n",
    "from main import run as invert\n",
    "\n",
    "from attention_maps_utils_by_timesteps import (\n",
    "    get_attn_maps,\n",
    "    cross_attn_init,\n",
    "    register_cross_attention_hook,\n",
    "    set_layer_with_name_and_path,\n",
    "    # preprocess,\n",
    "    # visualize_and_save_attn_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'safety_checker': None} are not expected by StableDiffusionXLImg2ImgPipeline and will be ignored.\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  9.26it/s]/cortex/users/yairshp/miniconda3/envs/renoise_inversion/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_type = Model_Type.SDXL_Turbo\n",
    "scheduler_type = Scheduler_Type.EULER\n",
    "pipe_inversion, pipe_inference = get_pipes(model_type, scheduler_type, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunConfig(\n",
    "    model_type=model_type,\n",
    "    scheduler_type=scheduler_type,\n",
    "    noise_regularization_lambda_kl=0.055,\n",
    "    noise_regularization_lambda_ac=10,\n",
    "    num_inversion_steps=4,\n",
    "    num_inference_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/object_placement_images.csv\"\n",
    ")\n",
    "data = pd.read_csv(data_path, dtype={\"bg_img_id\": str, \"ref_img_id\": str})\n",
    "open_images_path = \"/cortex/data/images/OpenImagesV6/images\"\n",
    "cocoee_path = \"/cortex/data/images/COCOEE/test_bench/Ref_3500\"\n",
    "\n",
    "# open_images dataset is used for bg_images. add a column of the path to the image (the path is the <open_images_path>/<image_id>.jpg)\n",
    "data[\"bg_img_path\"] = data[\"bg_img_id\"].apply(lambda x: f\"{open_images_path}/{x}.jpg\")\n",
    "\n",
    "# cocoee dataset is used for ref_images. add a column of the path to the image (the path is the <cocoee_path>/<image_id>_ref.png)\n",
    "data[\"ref_img_path\"] = data[\"ref_img_id\"].apply(lambda x: f\"{cocoee_path}/{x}_ref.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bg_images = [\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/bed.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/desk.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/cabinet.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/face.jpg\",\n",
    "]\n",
    "\n",
    "my_object_images = [\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/pillow/pillow.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/plant/plant.jpg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/vase/vase.jpeg\",\n",
    "    \"/home/lab/yairshp/projects/insert_object/benchmark/objects/hat/hat.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_images = list(data[\"bg_img_path\"])\n",
    "bg_images.extend(my_bg_images)\n",
    "ref_images = list(data[\"ref_img_path\"])\n",
    "ref_images.extend(my_object_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return image.convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "\n",
    "bg_images = [preprocess_image(image) for image in bg_images]\n",
    "ref_images = [preprocess_image(image) for image in ref_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_images(images, prompts):\n",
    "    inv_latents = []\n",
    "    noises = []\n",
    "    for image, prompt in zip(images, prompts):\n",
    "        _, inv_latent, noise, _ = invert(\n",
    "            image,\n",
    "            prompt,\n",
    "            config,\n",
    "            pipe_inversion=pipe_inversion,\n",
    "            pipe_inference=pipe_inference,\n",
    "            do_reconstruction=False,\n",
    "        )\n",
    "        inv_latents.append(inv_latent)\n",
    "        noises.append(noise)\n",
    "    return inv_latents, noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_prompts = [\"an image\" for _ in range(len(bg_images))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "inv_latents, noises = invert_images(bg_images, inversion_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_attn_init()\n",
    "pipe_inference.unet = set_layer_with_name_and_path(pipe_inference.unet)\n",
    "pipe_inference.unet = register_cross_attention_hook(pipe_inference.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt2tokens(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    tokens = []\n",
    "    for text_input_id in text_input_ids[0]:\n",
    "        token = tokenizer.decoder[text_input_id.item()]\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def visualize_and_save_attn_map(attn_map, tokenizer, prompt):\n",
    "    # match with tokens\n",
    "    tokens = prompt2tokens(tokenizer, prompt)\n",
    "    bos_token = tokenizer.bos_token\n",
    "    eos_token = tokenizer.eos_token\n",
    "    pad_token = tokenizer.pad_token\n",
    "    save_path = \"attn_maps\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    # to_pil = transforms.ToPILImage()\n",
    "    for i, (token, token_attn_map) in enumerate(zip(tokens, attn_map)):\n",
    "        if token != eos_token:\n",
    "            continue\n",
    "        token = token.replace(\"</w>\", \"\")\n",
    "        token = f\"{i}_<{token}>.jpg\"\n",
    "\n",
    "        # low quality\n",
    "        # to_pil(255 * token_attn_map).save(os.path.join(save_path, token))\n",
    "        # to_pil(255 * (token_attn_map - torch.min(token_attn_map)) / (torch.max(token_attn_map) - torch.min(token_attn_map))).save(os.path.join(save_path, token))\n",
    "\n",
    "        token_attn_map = token_attn_map.cpu().numpy()\n",
    "        normalized_token_attn_map = (\n",
    "            (token_attn_map - np.min(token_attn_map))\n",
    "            / (np.max(token_attn_map) - np.min(token_attn_map))\n",
    "            * 255\n",
    "        )\n",
    "        normalized_token_attn_map = normalized_token_attn_map.astype(np.uint8)\n",
    "        image = Image.fromarray(normalized_token_attn_map)\n",
    "        # image.save(os.path.join(save_path, token))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attn_map(timestep_attn_maps, tokenizer, prompt):\n",
    "    # max_height, max_width = 0, 0\n",
    "    preprocessed_attn_maps = {}\n",
    "    for k, v in timestep_attn_maps.items():\n",
    "        v = torch.mean(v.cpu(), axis=0).squeeze(0)\n",
    "        if v.shape[-1] != 16:\n",
    "            continue\n",
    "        # if len(v.shape) == 3:\n",
    "        #     _, h, w = v.shape\n",
    "        # else:\n",
    "        #     h, w = v.shape\n",
    "        # max_height = max(h, max_height)\n",
    "        # max_width = max(w, max_width)\n",
    "        # v = F.interpolate(\n",
    "        #     v.to(dtype=torch.float32).unsqueeze(0),\n",
    "        #     size=(max_height, max_width),\n",
    "        #     mode=\"bilinear\",\n",
    "        #     align_corners=False,\n",
    "        # ).squeeze(\n",
    "        #     0\n",
    "        # )  # (77,64,64)\n",
    "        preprocessed_attn_maps[k] = v\n",
    "        # attn_maps[k] = v\n",
    "\n",
    "    attn_map = torch.stack(list(preprocessed_attn_maps.values()), axis=0)\n",
    "    attn_map = torch.mean(attn_map, axis=0)\n",
    "\n",
    "    tokens = prompt2tokens(tokenizer, prompt)\n",
    "    eos_token = tokenizer.eos_token\n",
    "    for token, token_attn_map in zip(tokens, attn_map):\n",
    "        if token != eos_token:\n",
    "            continue\n",
    "        return token_attn_map\n",
    "\n",
    "    # return attn_map\n",
    "\n",
    "\n",
    "def get_attn_map(edit_prompt, tokenizer):\n",
    "    attn_maps = get_attn_maps()\n",
    "    attn_map = process_attn_map(attn_maps[-1], tokenizer, edit_prompt)\n",
    "    return attn_map\n",
    "    # attn_maps_images = []\n",
    "    # for attn_map in attn_maps:\n",
    "    #     attn_map = preprocess(attn_map, 512, 512)\n",
    "    #     attn_map_img = visualize_and_save_attn_map(\n",
    "    #         attn_map, pipe_inference.tokenizer, edit_prompt\n",
    "    #     )\n",
    "    #     attn_maps_images.append(attn_map_img)\n",
    "    # return attn_maps_images\n",
    "\n",
    "\n",
    "def reset_attn_maps():\n",
    "    attn_maps = get_attn_maps()\n",
    "    attn_maps.clear()\n",
    "\n",
    "\n",
    "def get_edit_images_and_attn_maps(inv_latents, noises, inversion_prompts, edit_prompts):\n",
    "    edit_images = []\n",
    "    per_timestep_attn_maps = []\n",
    "    for inv_latent, noise, inversion_prompt, edit_prompt in zip(\n",
    "        inv_latents, noises, inversion_prompts, edit_prompts\n",
    "    ):\n",
    "        pipe_inference.scheduler.set_noise_list(noise)\n",
    "        pipe_inference.cfg = config\n",
    "        edit_image = pipe_inference(\n",
    "            prompt=edit_prompt,\n",
    "            num_inference_steps=config.num_inference_steps,\n",
    "            negative_prompt=inversion_prompt,\n",
    "            image=inv_latent,\n",
    "            strength=config.inversion_max_step,\n",
    "            denoising_start=1.0 - config.inversion_max_step,\n",
    "            guidance_scale=config.guidance_scale,\n",
    "        ).images[0]\n",
    "        edit_images.append(edit_image)\n",
    "        per_timestep_attn_maps.append(\n",
    "            get_attn_map(edit_prompt, pipe_inference.tokenizer)\n",
    "        )\n",
    "        reset_attn_maps()\n",
    "\n",
    "    return edit_images, per_timestep_attn_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_prompts = inversion_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.31it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.78it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.81it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.83it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.76it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.67it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.65it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.76it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.79it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.73it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.65it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.77it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.36it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.64it/s]\n"
     ]
    }
   ],
   "source": [
    "edit_images, per_timestep_attn_maps = get_edit_images_and_attn_maps(\n",
    "    inv_latents, noises, inversion_prompts, edit_prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid_arr = []\n",
    "for bg_image, edit_image, sample_attn_maps in zip(\n",
    "    bg_images, edit_images, per_timestep_attn_maps\n",
    "):\n",
    "    image_grid_arr.append(bg_image)\n",
    "    # image_grid_arr.append(edit_image)\n",
    "    # for attn_map in sample_attn_maps:\n",
    "    #     image_grid_arr.append(attn_map)\n",
    "    image_grid_arr.append(sample_attn_maps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_maps_images = [\n",
    "#     Image.fromarray((attn_map.cpu().numpy() * 255))\n",
    "#     for attn_map in per_timestep_attn_maps\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_image_grid(image_grid_arr, len(bg_images), 2)\n",
    "make_image_grid(attn_maps_images, len(attn_maps_images), 1)\n",
    "# make_image_grid(image_grid_arr, len(bg_images), 2 + len(per_timestep_attn_maps[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, attn_maps in enumerate(per_timestep_attn_maps):\n",
    "#     attn_maps[-1].save(f\"attn_maps/{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = per_timestep_attn_maps[2].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(a, cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.072, 0.1198)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.min(), a.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
